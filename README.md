# ğŸ” RAG Historical Events Explorer

This is a Retrieval-Augmented Generation (RAG) system built to answer questions about 20th-century historical events. It features a sleek, responsive frontend built in Rust (Yew/WebAssembly) and a powerful backend built in Python (Flask).

The system retrieves relevant documents from a FAISS vector index (created from DBpedia data) and uses them as context for a Large Language Model (either local Ollama or OpenAI) to generate an accurate, grounded answer.

## âœ¨ Features

- Dual Backend Support: Seamlessly toggle between a local Ollama model (phi3:mini) or the remote OpenAI API (gpt-4-turbo).

- Adjustable Context: Use a slider to control the number of retrieved documents (top_k) to be fed to the model.

- Semantic Search: Uses sentence-transformers and FAISS to perform fast, accurate semantic search over the historical event database.

- Rich Data Source: The knowledge base is built from 20th-century historical events pulled from DBpedia using SPARQL.

- Modern Rust Frontend: A fast, fully client-side application built with Yew, which compiles to WebAssembly.

## ğŸ› ï¸ Tech Stack

| Component | Technology | Description |
| :--- | :--- | :--- |
| **Frontend** | [Rust](https://www.rust-lang.org/) + [Yew](https://yew.rs/) | A modern framework for building multi-threaded client-side web apps with WebAssembly. |
| **Frontend** | [Trunk](https://trunkrs.dev/) | Build tool for Yew applications. |
| **Frontend** | [SASS/SCSS](https://sass-lang.com/) | For advanced and clean styling. |
| **Backend** | [Python](https://www.python.org/) + [Flask](https://flask.palletsprojects.com/) | A lightweight web server to host the RAG API. |
| **RAG Pipeline** | [Ollama (`phi3:mini`)](https://ollama.com/) | The default, local LLM for generation. |
| **RAG Pipeline** | [OpenAI (`gpt-4-turbo`)](https://openai.com/) | The optional, high-performance LLM. |
| **RAG Pipeline** | [FAISS](https://faiss.ai/) | A library for efficient similarity search in vector databases. |
| **RAG Pipeline** | [`sentence-transformers`](https://sbert.net/) | For generating high-quality text embeddings (`all-MiniLM-L6-v2`). |

## ğŸš€ Setup and Installation

Follow these steps to get the complete application running on your local machine.

### 1. Prerequisites

Before you begin, ensure you have the following tools installed:

- Python 3.10+

- Rust & Cargo

- Trunk: The build tool for Yew. Install it via Cargo:

```Bash
    cargo install trunk
```

- Ollama: Download and install the desktop application.

### 2. Backend Setup

This will set up the Python server, download the ML models, and gather the data.

#### 1.Clone the repository (if you haven't):

```Bash
git clone https://github.com/frontinus/Generative_IA_Wiki.git
cd Generative_IA_Wiki
```

#### 2.Create a virtual environment and install dependencies:

```Bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

#### 3.Download the Ollama LLM: Pull the phi3:mini model that the pipeline uses by default.

```Bash
ollama pull phi3:mini
```

#### 4.Gather Data from DBpedia:

Run the data gathering script. This will create the historical_events_with_abstracts.csv file.

```Bash
python data_gathering.py
```

#### 5.(Optional) Set OpenAI API Key:

If you want to use the OpenAI toggle, set your API key as an environment variable.

```Bash
export OPENAI_API_KEY="sk-YourActualApiKeyHere"
```

### 3. Frontend Setup

The frontend is in a separate directory (small_interface) and requires no additional setup, as trunk will handle all dependencies.

## â–¶ï¸ How to Run

You will need three separate terminals running at the same time.

### Terminal 1: Run Ollama

Ensure the Ollama Desktop application is running. Alternatively, you can run the service from your terminal:

```Bash
ollama serve
```

### Terminal 2: Run the Flask Backend (API)

This terminal runs the Python server, which will load the FAISS index into memory and serve the RAG API.

```Bash
# Navigate to the project root
cd Generative_IA_Wiki

# Activate your virtual environment
source venv/bin/activate

# Run the app
python app.py
```

Your backend is now running at http://127.0.0.1:8000.

### Terminal 3: Run the Yew Frontend (UI)

This terminal builds and serves the Rust-based user interface.

```Bash
# Navigate to the frontend directory
cd small_interface

# Serve the app. Trunk will compile and open your browser.
trunk serve --open
```

Your frontend is now running at http://127.0.0.1:8080/. You can now interact with the application!

## ğŸ“ Project Structure

```markdown
.
â”œâ”€â”€ app.py                  # Flask backend server
â”œâ”€â”€ pipeline.py             # RAG logic (FAISS, Ollama, OpenAI)
â”œâ”€â”€ data_gathering.py       # SPARQL script to get data from DBpedia
â”œâ”€â”€ requirements.txt        # (Assumed) Python dependencies
â”œâ”€â”€ historical_events_with_abstracts.csv  # (Generated by data_gathering.py)
â”‚
â”œâ”€â”€ small_interface/        # Yew frontend folder
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ main.rs         # Yew app logic
â”‚   â”œâ”€â”€ index.html          # Entrypoint for frontend
â”‚   â””â”€â”€ styles.scss         # App styling
â”‚
â””â”€â”€ README.md               # This file
```

## ğŸ‘¨â€ğŸ’» Authors

This project was built by:

* **Francesco**
  * [GitHub](https://github.com/frontinus)
  * [LinkedIn](https://linkedin.com/in/francesco-abate-79601719b)
* **Thomas**
  * [GitHub](https://github.com/thetom061)
  * [LinkedIn](https://www.linkedin.com/in/thomas-cotte-9870531a1/)